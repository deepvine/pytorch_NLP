{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import vocab\n",
    "from torchtext import data\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator\n",
    "from torchtext.vocab import GloVe\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    return x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "max_vocab = 8000\n",
    "fix_length=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize=tokenizer, pad_token=BLANK_WORD, lower=True, batch_first=True, fix_length=fix_length)\n",
    "LABEL = Field(sequential=False, unk_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset(path='./text_emotion.csv', \n",
    "                            format='csv', \n",
    "                            skip_header=True,\n",
    "                            fields=[(\"tweet_id\", None),(\"sentiment\", LABEL),(\"author\", None),(\"content\",TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310fa3380d274ca1928528e646f89692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove = vocab.Vectors('data/glove.6B.300d.txt')\n",
    "tqdm_notebook().pandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec = vocab.Vectors('glove.twitter.27B.100d.txt', './data/glove_embedding/')\n",
    "#https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(train_data, max_size=max_vocab)\n",
    "TEXT.build_vocab(train_data, min_freq=3)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.set_vectors(glove.stoi, glove.vectors, dim=300)\n",
    "TEXT.fix_length = fix_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@tiffanylue', 'i', 'know']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_example = train_data.examples[0]\n",
    "one_example.content[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_loader = Iterator(train_data, \n",
    "                        batch_size=64, \n",
    "                        device=-1, \n",
    "                        repeat=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 30])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break;\n",
    "print(batch.content.shape)\n",
    "print(batch.sentiment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<blank>',\n",
       " 'i',\n",
       " 'to',\n",
       " 'the',\n",
       " 'a',\n",
       " 'my',\n",
       " 'and',\n",
       " 'you',\n",
       " 'is',\n",
       " 'in',\n",
       " 'for',\n",
       " 'it',\n",
       " 'of',\n",
       " 'on',\n",
       " 'have',\n",
       " 'so',\n",
       " 'that',\n",
       " 'me',\n",
       " 'but',\n",
       " 'just',\n",
       " \"i'm\",\n",
       " 'with',\n",
       " 'be',\n",
       " 'at',\n",
       " 'was',\n",
       " 'not',\n",
       " 'day',\n",
       " 'this',\n",
       " 'all',\n",
       " 'get',\n",
       " 'good',\n",
       " 'like',\n",
       " 'are',\n",
       " 'out',\n",
       " 'up',\n",
       " '-',\n",
       " \"it's\",\n",
       " 'your',\n",
       " 'go',\n",
       " 'no',\n",
       " 'got',\n",
       " 'now',\n",
       " 'going',\n",
       " 'love',\n",
       " 'do',\n",
       " 'from',\n",
       " 'happy',\n",
       " 'will',\n",
       " 'work',\n",
       " 'im',\n",
       " 'what',\n",
       " 'we',\n",
       " \"don't\",\n",
       " 'about',\n",
       " 'u',\n",
       " 'one',\n",
       " 'really',\n",
       " 'back',\n",
       " 'its',\n",
       " 'too',\n",
       " 'am',\n",
       " 'had',\n",
       " 'see',\n",
       " 'can',\n",
       " 'know',\n",
       " 'some',\n",
       " \"can't\",\n",
       " 'if',\n",
       " 'time',\n",
       " 'new',\n",
       " 'when',\n",
       " 'as',\n",
       " 'lol',\n",
       " 'want',\n",
       " 'think',\n",
       " 'how',\n",
       " '&amp;',\n",
       " 'still',\n",
       " 'an',\n",
       " 'today',\n",
       " 'they',\n",
       " 'miss',\n",
       " 'last',\n",
       " '2',\n",
       " 'more',\n",
       " 'off',\n",
       " 'need',\n",
       " 'oh',\n",
       " 'hope',\n",
       " 'has',\n",
       " 'there',\n",
       " 'been',\n",
       " 'home',\n",
       " 'much',\n",
       " 'feel',\n",
       " 'thanks',\n",
       " 'night',\n",
       " 'great',\n",
       " 'only',\n",
       " 'or',\n",
       " 'would',\n",
       " 'he',\n",
       " 'her',\n",
       " 'wish',\n",
       " 'then',\n",
       " 'well',\n",
       " 'why',\n",
       " 'very',\n",
       " 'here',\n",
       " \"i'll\",\n",
       " 'by',\n",
       " 'make',\n",
       " 'gonna',\n",
       " 'she',\n",
       " 'did',\n",
       " \"that's\",\n",
       " 'getting',\n",
       " 'twitter',\n",
       " 'morning',\n",
       " 'should',\n",
       " 'could',\n",
       " \"mother's\",\n",
       " 'bad',\n",
       " 'haha',\n",
       " 'fun',\n",
       " 'sorry',\n",
       " 'mothers',\n",
       " 'sad',\n",
       " 'nice',\n",
       " 'come',\n",
       " \"didn't\",\n",
       " 'next',\n",
       " 'right',\n",
       " \"i've\",\n",
       " 'over',\n",
       " 'way',\n",
       " 'better',\n",
       " \"you're\",\n",
       " 'were',\n",
       " 'watching',\n",
       " 'dont',\n",
       " 'them',\n",
       " 'hate',\n",
       " 'after',\n",
       " '...',\n",
       " 'even',\n",
       " 'never',\n",
       " 'people',\n",
       " 'tomorrow',\n",
       " 'being',\n",
       " 'thank',\n",
       " 'little',\n",
       " 'having',\n",
       " 'sleep',\n",
       " 'his',\n",
       " 'say',\n",
       " 'again',\n",
       " 'take',\n",
       " 'best',\n",
       " 'cant',\n",
       " 'week',\n",
       " 'wait',\n",
       " 'who',\n",
       " 'our',\n",
       " 'long',\n",
       " 'than',\n",
       " 'any',\n",
       " 'him',\n",
       " 'it.',\n",
       " 'working',\n",
       " 'wanna',\n",
       " 'always',\n",
       " 'tonight',\n",
       " 'ur',\n",
       " '4',\n",
       " 'because',\n",
       " 'though',\n",
       " 'days',\n",
       " 'look',\n",
       " 'may',\n",
       " 'find',\n",
       " '!',\n",
       " 'school',\n",
       " 'bed',\n",
       " 'first',\n",
       " 'another',\n",
       " 'made',\n",
       " 'down',\n",
       " 'feeling',\n",
       " 'doing',\n",
       " 'hey',\n",
       " 'sure',\n",
       " '?',\n",
       " 'watch',\n",
       " 'weekend',\n",
       " 'where',\n",
       " 'trying',\n",
       " 'show',\n",
       " 'thing',\n",
       " 'yeah',\n",
       " 'looking',\n",
       " '.',\n",
       " 'mom',\n",
       " 'please',\n",
       " 'day!',\n",
       " '3',\n",
       " 'ready',\n",
       " 'let',\n",
       " \"won't\",\n",
       " 'pretty',\n",
       " 'before',\n",
       " 'something',\n",
       " 'went',\n",
       " 'awesome',\n",
       " '@',\n",
       " 'done',\n",
       " 'phone',\n",
       " 'into',\n",
       " 'thought',\n",
       " 'old',\n",
       " 'me.',\n",
       " 'everyone',\n",
       " 'friends',\n",
       " 'live',\n",
       " 'maybe',\n",
       " 'now.',\n",
       " 'big',\n",
       " \"doesn't\",\n",
       " 'us',\n",
       " 'keep',\n",
       " 'looks',\n",
       " 'finally',\n",
       " 'hours',\n",
       " 'missed',\n",
       " 'movie',\n",
       " '&lt;3',\n",
       " 'today.',\n",
       " 'guess',\n",
       " 'guys',\n",
       " 'x',\n",
       " 'left',\n",
       " 'other',\n",
       " 'someone',\n",
       " 'already',\n",
       " 'sick',\n",
       " 'follow',\n",
       " 'bit',\n",
       " 'life',\n",
       " 'same',\n",
       " 'glad',\n",
       " 'saw',\n",
       " 'tell',\n",
       " 'those',\n",
       " 'star',\n",
       " 'two',\n",
       " 'hear',\n",
       " 'said',\n",
       " 'such',\n",
       " 'try',\n",
       " 'nothing',\n",
       " 'ok',\n",
       " 'makes',\n",
       " 'you!',\n",
       " 'house',\n",
       " 'might',\n",
       " 'while',\n",
       " 'until',\n",
       " 'ever',\n",
       " 'tired',\n",
       " 'cool',\n",
       " 'damn',\n",
       " 'you.',\n",
       " 'call',\n",
       " 'hot',\n",
       " 'it!',\n",
       " 'things',\n",
       " 'soon',\n",
       " 'start',\n",
       " 'waiting',\n",
       " 'since',\n",
       " 'few',\n",
       " 'coming',\n",
       " 'friend',\n",
       " 'day.',\n",
       " 'gotta',\n",
       " 'thats',\n",
       " 'actually',\n",
       " 'hard',\n",
       " 'lost',\n",
       " 'must',\n",
       " '1',\n",
       " 'also',\n",
       " 'yes',\n",
       " 'away',\n",
       " 'poor',\n",
       " '5',\n",
       " 'making',\n",
       " 'n',\n",
       " 'their',\n",
       " 'play',\n",
       " 'put',\n",
       " 'till',\n",
       " '..',\n",
       " 'girl',\n",
       " 'does',\n",
       " 'give',\n",
       " 'tweet',\n",
       " 'friday',\n",
       " 'sun',\n",
       " 'found',\n",
       " 'man',\n",
       " 'read',\n",
       " 'amazing',\n",
       " 'head',\n",
       " 'without',\n",
       " 'car',\n",
       " \"he's\",\n",
       " 'around',\n",
       " 'year',\n",
       " 'almost',\n",
       " 'check',\n",
       " 'early',\n",
       " 'enjoy',\n",
       " 'help',\n",
       " 'least',\n",
       " \"isn't\",\n",
       " 'listening',\n",
       " 'sucks',\n",
       " 'totally',\n",
       " 'talk',\n",
       " 'wanted',\n",
       " 'hi',\n",
       " 'yeah,',\n",
       " 'yet',\n",
       " \"haven't\",\n",
       " \"i'd\",\n",
       " 'omg',\n",
       " 'song',\n",
       " 'too.',\n",
       " 'leave',\n",
       " 'many',\n",
       " 'believe',\n",
       " 'lot',\n",
       " 'missing',\n",
       " 'most',\n",
       " 'sounds',\n",
       " 'use',\n",
       " 'forward',\n",
       " 'anyone',\n",
       " 'thinking',\n",
       " 'hair',\n",
       " 'finished',\n",
       " 'myself',\n",
       " 'probably',\n",
       " ';)',\n",
       " 'baby',\n",
       " 'cause',\n",
       " 'monday',\n",
       " 'birthday',\n",
       " 'gone',\n",
       " 'ya',\n",
       " 'aww',\n",
       " 'weather',\n",
       " 'anything',\n",
       " 'playing',\n",
       " 'every',\n",
       " 'stuff',\n",
       " 'free',\n",
       " 'rain',\n",
       " 'these',\n",
       " 'well,',\n",
       " 'family',\n",
       " 'job',\n",
       " 'through',\n",
       " 'eat',\n",
       " 'didnt',\n",
       " 'excited',\n",
       " 'later',\n",
       " ',',\n",
       " 'everything',\n",
       " 'god',\n",
       " 'late',\n",
       " 'mean',\n",
       " 'stupid',\n",
       " 'lol.',\n",
       " \"we're\",\n",
       " 'which',\n",
       " 'party',\n",
       " 'stop',\n",
       " 'eating',\n",
       " 'welcome',\n",
       " 'beautiful',\n",
       " 'stuck',\n",
       " 'took',\n",
       " 'whole',\n",
       " 'end',\n",
       " 'lunch',\n",
       " 'xx',\n",
       " 'bored',\n",
       " \"wasn't\",\n",
       " 'hour',\n",
       " 'kinda',\n",
       " 'woke',\n",
       " 'says',\n",
       " 'wants',\n",
       " 'sooo',\n",
       " 'ill',\n",
       " 'lovely',\n",
       " 'music',\n",
       " 'buy',\n",
       " 'dinner',\n",
       " 'seen',\n",
       " 'came',\n",
       " 'it,',\n",
       " \"she's\",\n",
       " 'summer',\n",
       " 'luck',\n",
       " 'stay',\n",
       " 'yay',\n",
       " 'money',\n",
       " 'r',\n",
       " 'taking',\n",
       " 'game',\n",
       " 'meet',\n",
       " \"there's\",\n",
       " 'able',\n",
       " 'mine',\n",
       " 'sunday',\n",
       " '=',\n",
       " 'hurts',\n",
       " 'me!',\n",
       " 'real',\n",
       " 'work.',\n",
       " 'cold',\n",
       " 'loved',\n",
       " 'sitting',\n",
       " 'world',\n",
       " 'super',\n",
       " \"couldn't\",\n",
       " 'cute',\n",
       " 'forgot',\n",
       " 'following',\n",
       " \"they're\",\n",
       " 'enough',\n",
       " 'seems',\n",
       " 'too!',\n",
       " 'you?',\n",
       " 'far',\n",
       " 'good.',\n",
       " 'used',\n",
       " 'tomorrow.',\n",
       " 'coffee',\n",
       " 'gets',\n",
       " 'kids',\n",
       " 'lots',\n",
       " 'told',\n",
       " 'hit',\n",
       " 'place',\n",
       " 'quite',\n",
       " 'send',\n",
       " 'ugh',\n",
       " 'food',\n",
       " 'shit',\n",
       " 'me,',\n",
       " 'sweet',\n",
       " 'talking',\n",
       " 'today,',\n",
       " 'both',\n",
       " 'hopefully',\n",
       " 'moms',\n",
       " 'full',\n",
       " 'half',\n",
       " 'time.',\n",
       " 'wars',\n",
       " \"what's\",\n",
       " 'fucking',\n",
       " 'outside',\n",
       " 'wont',\n",
       " 'wow',\n",
       " 'you,',\n",
       " 'home.',\n",
       " 'seeing',\n",
       " 'watched',\n",
       " 'busy',\n",
       " 'cuz',\n",
       " 'feels',\n",
       " 'room',\n",
       " 'up.',\n",
       " 'yesterday',\n",
       " 'again.',\n",
       " 'funny',\n",
       " 'now,',\n",
       " 'own',\n",
       " '10',\n",
       " 'haha.',\n",
       " 'office',\n",
       " 'video',\n",
       " 'weeks',\n",
       " 'bank',\n",
       " 'heard',\n",
       " 'out.',\n",
       " 'remember',\n",
       " 'soo',\n",
       " 'though.',\n",
       " 'times',\n",
       " 'night.',\n",
       " 'post',\n",
       " 'starting',\n",
       " 'called',\n",
       " 'dad',\n",
       " 'name',\n",
       " 'tonight.',\n",
       " 'awww',\n",
       " 'oh,',\n",
       " 'that.',\n",
       " 'else',\n",
       " 'tried',\n",
       " 'guy',\n",
       " 'internet',\n",
       " 'tv',\n",
       " 'yes,',\n",
       " 'headache',\n",
       " 'change',\n",
       " 'kind',\n",
       " 'pic',\n",
       " 'yay!',\n",
       " 'tho',\n",
       " 'today!',\n",
       " 'holiday',\n",
       " 'rest',\n",
       " 'turn',\n",
       " 'years',\n",
       " '+',\n",
       " 'alone',\n",
       " 'here.',\n",
       " 'leaving',\n",
       " 'thanks!',\n",
       " '!!',\n",
       " 'b',\n",
       " 'bought',\n",
       " 'part',\n",
       " 'til',\n",
       " 'w/',\n",
       " 'wonderful',\n",
       " 'wrong',\n",
       " 'day,',\n",
       " 'dog',\n",
       " 'shopping',\n",
       " 'wishing',\n",
       " '6',\n",
       " 'broke',\n",
       " 'goodnight',\n",
       " 'hurt',\n",
       " 'once',\n",
       " 'soooo',\n",
       " ')',\n",
       " 'book',\n",
       " 'break',\n",
       " 'followers',\n",
       " 'hahaha',\n",
       " 'hello',\n",
       " 'high',\n",
       " 'hoping',\n",
       " 'minutes',\n",
       " 'mum',\n",
       " 'class',\n",
       " 'drink',\n",
       " 'heading',\n",
       " 'cry',\n",
       " 'iphone',\n",
       " 'loves',\n",
       " 'drive',\n",
       " 'run',\n",
       " 'there.',\n",
       " 'started',\n",
       " 'comes',\n",
       " 'computer',\n",
       " 'girls',\n",
       " 'ha',\n",
       " 'ice',\n",
       " 'there!',\n",
       " 'bout',\n",
       " 'couple',\n",
       " 'hell',\n",
       " 'needs',\n",
       " 'set',\n",
       " 'close',\n",
       " 'favorite',\n",
       " 'now!',\n",
       " 'tweets',\n",
       " 'wake',\n",
       " 'definitely',\n",
       " 'goes',\n",
       " 'lil',\n",
       " 'person',\n",
       " 'running',\n",
       " 'ask',\n",
       " 'heart',\n",
       " 'raining',\n",
       " 'supposed',\n",
       " '4th',\n",
       " 'gave',\n",
       " 'ive',\n",
       " 'move',\n",
       " 'saturday',\n",
       " 'sometimes',\n",
       " 'trek',\n",
       " 'weekend.',\n",
       " 'writing',\n",
       " 'xd',\n",
       " 'morning!',\n",
       " \"you'll\",\n",
       " '@tommcfly',\n",
       " 'card',\n",
       " 'care',\n",
       " 'catch',\n",
       " 'link',\n",
       " 'night,',\n",
       " 'online',\n",
       " 'perfect',\n",
       " 'spend',\n",
       " 'write',\n",
       " \"aren't\",\n",
       " 'blog',\n",
       " 'crazy',\n",
       " 'face',\n",
       " 'instead',\n",
       " 'mind',\n",
       " 'trip',\n",
       " '7',\n",
       " 'win',\n",
       " 'boy',\n",
       " 'breakfast',\n",
       " 'dead',\n",
       " 'facebook',\n",
       " 'fun!',\n",
       " 'lucky',\n",
       " 'means',\n",
       " 'sleep.',\n",
       " 'soon.',\n",
       " 'sore',\n",
       " 'anymore',\n",
       " 'concert',\n",
       " 'enjoying',\n",
       " 'hehe',\n",
       " 'text',\n",
       " 'due',\n",
       " 'goin',\n",
       " 'idea',\n",
       " 'okay',\n",
       " 'picture',\n",
       " 'sunny',\n",
       " \"we'll\",\n",
       " 'boo',\n",
       " 'cut',\n",
       " 'mother',\n",
       " 'tea',\n",
       " 'bed.',\n",
       " 'fan',\n",
       " 'it?',\n",
       " 'less',\n",
       " 'moving',\n",
       " 'no,',\n",
       " 'reading',\n",
       " 'well.',\n",
       " 'all.',\n",
       " 'asleep',\n",
       " 'bring',\n",
       " 'chocolate',\n",
       " 'either',\n",
       " 'hang',\n",
       " 'me?',\n",
       " 'pics',\n",
       " 'top',\n",
       " 'week.',\n",
       " 'xxx',\n",
       " 'ah',\n",
       " 'email',\n",
       " 'fuck',\n",
       " 'june',\n",
       " 'reply',\n",
       " 'second',\n",
       " 'using',\n",
       " '!!!',\n",
       " '8',\n",
       " '@mitchelmusso',\n",
       " 'finish',\n",
       " 'la',\n",
       " 'morning.',\n",
       " 'news',\n",
       " 'seem',\n",
       " 'doesnt',\n",
       " 'driving',\n",
       " 'fell',\n",
       " 'ipod',\n",
       " 'liked',\n",
       " 'ones',\n",
       " 'red',\n",
       " 'saying',\n",
       " 'spent',\n",
       " 'test',\n",
       " 'vote',\n",
       " 'awake',\n",
       " 'black',\n",
       " 'cleaning',\n",
       " 'forget',\n",
       " 'sent',\n",
       " 'suck',\n",
       " 'exam',\n",
       " 'laptop',\n",
       " 'meeting',\n",
       " 'mommy',\n",
       " 'night!',\n",
       " 'sad.',\n",
       " 'work,',\n",
       " 'worth',\n",
       " \"you've\",\n",
       " '1st',\n",
       " 'beach',\n",
       " 'cat',\n",
       " 'dream',\n",
       " 'happened',\n",
       " 'movies',\n",
       " 'past',\n",
       " 'sound',\n",
       " 'works',\n",
       " 'youtube',\n",
       " 'ass',\n",
       " 'mad',\n",
       " 'months',\n",
       " 'site',\n",
       " 'special',\n",
       " 'under',\n",
       " \"wouldn't\",\n",
       " '....',\n",
       " 'city',\n",
       " 'da',\n",
       " 'listen',\n",
       " 'lol!',\n",
       " 'open',\n",
       " 'rather',\n",
       " 'sleeping',\n",
       " 'that!',\n",
       " 'wow,',\n",
       " 'ate',\n",
       " 'fine',\n",
       " 'fun.',\n",
       " 'knew',\n",
       " 'month',\n",
       " 'nite',\n",
       " 'tickets',\n",
       " 'up!',\n",
       " 'up,',\n",
       " ':',\n",
       " 'aw',\n",
       " 'final',\n",
       " 'pain',\n",
       " ':/',\n",
       " '@mileycyrus',\n",
       " 'bye',\n",
       " 'clean',\n",
       " 'date',\n",
       " 'havent',\n",
       " 'know,',\n",
       " 'lol,',\n",
       " 'played',\n",
       " 'ppl',\n",
       " 'rock',\n",
       " 'yea',\n",
       " 'except',\n",
       " 'good!',\n",
       " 'lmao',\n",
       " 'problem',\n",
       " 'whats',\n",
       " 'broken',\n",
       " 'different',\n",
       " 'episode',\n",
       " 'eyes',\n",
       " 'huge',\n",
       " 'looked',\n",
       " 'loving',\n",
       " 'reason',\n",
       " 'ride',\n",
       " 'save',\n",
       " 'short',\n",
       " 'shower',\n",
       " 'walk',\n",
       " 'warm',\n",
       " 'wear',\n",
       " 'awesome!',\n",
       " 'google',\n",
       " 'jonas',\n",
       " \"mom's\",\n",
       " 'add',\n",
       " 'again,',\n",
       " 'completely',\n",
       " 'died',\n",
       " 'do.',\n",
       " 'drinking',\n",
       " 'fall',\n",
       " 'figure',\n",
       " 'hey,',\n",
       " 'interesting',\n",
       " 'know.',\n",
       " 'plans',\n",
       " 'sadly',\n",
       " 'season',\n",
       " 'sister',\n",
       " 'three',\n",
       " '30',\n",
       " 'boring',\n",
       " 'inside',\n",
       " 'ok,',\n",
       " 'shows',\n",
       " 'upset',\n",
       " 'water',\n",
       " 'apparently',\n",
       " 'boys',\n",
       " 'evening',\n",
       " 'pictures',\n",
       " 'slept',\n",
       " 'songs',\n",
       " 'true',\n",
       " 'white',\n",
       " 'bad.',\n",
       " 'btw',\n",
       " 'dance',\n",
       " 'during',\n",
       " 'especially',\n",
       " 'fail',\n",
       " \"let's\",\n",
       " 'list',\n",
       " 'living',\n",
       " 'meant',\n",
       " 'one.',\n",
       " 'out!',\n",
       " 'park',\n",
       " 'plan',\n",
       " 'cake',\n",
       " 'each',\n",
       " 'learn',\n",
       " 'near',\n",
       " 'pizza',\n",
       " 'seriously',\n",
       " 'study',\n",
       " 'tired.',\n",
       " 'uk',\n",
       " '--',\n",
       " 'ago',\n",
       " 'sick.',\n",
       " 'together',\n",
       " 'tonight,',\n",
       " 'tour',\n",
       " 'visit',\n",
       " '??',\n",
       " 'brother',\n",
       " 'chance',\n",
       " 'dude',\n",
       " 'good,',\n",
       " 'gym',\n",
       " 'haha,',\n",
       " 'hanging',\n",
       " 'hungry',\n",
       " 'mood',\n",
       " 'shame',\n",
       " 'worst',\n",
       " 'absolutely',\n",
       " 'again!',\n",
       " 'answer',\n",
       " 'course',\n",
       " 'crap',\n",
       " 'felt',\n",
       " 'hates',\n",
       " 'join',\n",
       " 'time!',\n",
       " 'tomorrow!',\n",
       " 'too,',\n",
       " 'town',\n",
       " '@ddlovato',\n",
       " 'back.',\n",
       " 'bday',\n",
       " 'cream',\n",
       " 'exams',\n",
       " 'here!',\n",
       " 'kill',\n",
       " 'man,',\n",
       " 'met',\n",
       " 'morning,',\n",
       " 'realized',\n",
       " 'school.',\n",
       " 'sign',\n",
       " 'so,',\n",
       " 'starts',\n",
       " 'them.',\n",
       " 'weird',\n",
       " '/',\n",
       " '2nd',\n",
       " 'account',\n",
       " 'beer',\n",
       " 'better.',\n",
       " 'cannot',\n",
       " 'english',\n",
       " 'fans',\n",
       " 'fast',\n",
       " 'happen',\n",
       " 'needed',\n",
       " 'officially',\n",
       " 'plus',\n",
       " 'side',\n",
       " 'thinks',\n",
       " 'tonight!',\n",
       " 'traffic',\n",
       " 'awesome.',\n",
       " 'chicken',\n",
       " 'cos',\n",
       " 'film',\n",
       " 'haha!',\n",
       " \"it'll\",\n",
       " 'jealous',\n",
       " 'know!',\n",
       " 'no!',\n",
       " 'page',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'slow',\n",
       " 'staying',\n",
       " 'thanks.',\n",
       " 'wishes',\n",
       " 'bet',\n",
       " 'bus',\n",
       " 'dear',\n",
       " 'fact',\n",
       " 'guitar',\n",
       " 'lazy',\n",
       " 'london',\n",
       " 'pick',\n",
       " 'ran',\n",
       " 'sat',\n",
       " 'small',\n",
       " 'sucks.',\n",
       " 'that,',\n",
       " 'throat',\n",
       " 'weekend!',\n",
       " 'wonder',\n",
       " 'worse',\n",
       " 'ahh',\n",
       " 'aint',\n",
       " 'appreciate',\n",
       " 'bless',\n",
       " 'business',\n",
       " 'camera',\n",
       " 'club',\n",
       " 'cool.',\n",
       " 'die',\n",
       " 'earlier',\n",
       " 'feet',\n",
       " 'green',\n",
       " 'keeps',\n",
       " 'lady',\n",
       " 'longer',\n",
       " 'middle',\n",
       " 'moment',\n",
       " 'much.',\n",
       " 'nap',\n",
       " 'ohh',\n",
       " 'parents',\n",
       " 'pay',\n",
       " 'posted',\n",
       " 'relaxing',\n",
       " 'sunshine',\n",
       " 'there,',\n",
       " 'tummy',\n",
       " 'walking',\n",
       " '@jonasbrothers',\n",
       " 'along',\n",
       " 'although',\n",
       " 'body',\n",
       " 'gift',\n",
       " 'idk',\n",
       " 'message',\n",
       " 'none',\n",
       " 'now...',\n",
       " 'power',\n",
       " 'spending',\n",
       " 'tweeting',\n",
       " 'windows',\n",
       " '12',\n",
       " '@jonathanrknight',\n",
       " 'congrats',\n",
       " 'drunk',\n",
       " 'luv',\n",
       " 'out,',\n",
       " 'radio',\n",
       " 'soon!',\n",
       " 'stomach',\n",
       " 'though!',\n",
       " 'twitter.',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    for name, w in model.named_parameters():\n",
    "        if not exclude in name:\n",
    "            if 'weight' in name:\n",
    "                if method is 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method is 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0.0)\n",
    "            else: \n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model, ignore='embedding'):\n",
    "    total = 0\n",
    "    for name, w in model.named_parameters():\n",
    "        if not ignore or ignore not in name:\n",
    "            total += w.nelement()\n",
    "            print('{} : {}  {} parameters'.format(name, w.shape, w.nelement()))\n",
    "    print('-------'*4)\n",
    "    print('Total {} parameters'.format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1024\n",
    "epochs=200\n",
    "embidding_dim = 300\n",
    "seq_length = 50\n",
    "vocab_size = len(TEXT.vocab.itos)\n",
    "num_filters = 128\n",
    "kernel_sizes = [3,4,5]\n",
    "hidden_dim = 128 # hidden size of fully conntected layer\n",
    "label_size = len(LABEL.vocab)\n",
    "print_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    #num_filters = out-channels\n",
    "    def __init__(self, lm, padding_idx, vocab_size, embedding_dim, num_filters, kernel_sizes, num_classes, dropout_prob):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        #self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(lm)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(kernel_sizes[0], embedding_dim))\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(kernel_sizes[1], embedding_dim))\n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(kernel_sizes[2], embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(kernel_sizes)*num_filters, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        #x = [batch size, sent len]\n",
    "        x = inputs\n",
    "\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        embedded = self.embedding(x)\n",
    "        #print(embedded.shape)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #print(embedded.shape)\n",
    "        \n",
    "        #conv_n = [batch size, n_filters, sent_len - filter_sizes[n]]\n",
    "        conved1 = F.relu(self.conv1(embedded).squeeze(3))\n",
    "        conved2 = F.relu(self.conv2(embedded).squeeze(3))\n",
    "        conved3 = F.relu(self.conv3(embedded).squeeze(3))\n",
    "        #print(conved11.shape)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        pooled1 = F.max_pool1d(conved1, conved1.shape[2]).squeeze(2)\n",
    "        pooled2 = F.max_pool1d(conved2, conved2.shape[2]).squeeze(2)\n",
    "        pooled3 = F.max_pool1d(conved3, conved3.shape[2]).squeeze(2)\n",
    "        #print(pooled11.shape)\n",
    "        \n",
    "        \n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        cat = self.dropout(torch.cat((pooled1, pooled2, pooled3), dim=1))\n",
    "        #print(cat.shape)\n",
    "        \n",
    "        fc = self.fc(cat)\n",
    "        return fc\n",
    "        #return F.log_softmax(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용할 수 있는지 확인\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n",
    "#train_on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(11631, 300, padding_idx=1)\n",
       "  (conv1): Conv2d(1, 128, kernel_size=(3, 300), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 128, kernel_size=(4, 300), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 128, kernel_size=(5, 300), stride=(1, 1))\n",
       "  (fc): Linear(in_features=384, out_features=13, bias=True)\n",
       "  (dropout): Dropout(p=0.1)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextCNN(TEXT.vocab.vectors, TEXT.vocab.stoi[TEXT.pad_token], vocab_size, embidding_dim, num_filters, kernel_sizes, label_size, 0.1)\n",
    "init_network(model)\n",
    "if(train_on_gpu):\n",
    "    model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight : torch.Size([11631, 300])  3489300 parameters\n",
      "conv1.weight : torch.Size([128, 1, 3, 300])  115200 parameters\n",
      "conv1.bias : torch.Size([128])  128 parameters\n",
      "conv2.weight : torch.Size([128, 1, 4, 300])  153600 parameters\n",
      "conv2.bias : torch.Size([128])  128 parameters\n",
      "conv3.weight : torch.Size([128, 1, 5, 300])  192000 parameters\n",
      "conv3.bias : torch.Size([128])  128 parameters\n",
      "fc.weight : torch.Size([13, 384])  4992 parameters\n",
      "fc.bias : torch.Size([13])  13 parameters\n",
      "----------------------------\n",
      "Total 3955489 parameters\n"
     ]
    }
   ],
   "source": [
    "criterion = F.cross_entropy\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3,momentum=0.8)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "print_model(model, ignore=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200... Step: 1000... Loss: 2.0054...\n",
      "Epoch: 2/200... Step: 2000... Loss: 1.9270...\n",
      "Epoch: 3/200... Step: 3000... Loss: 1.7252...\n",
      "Epoch: 4/200... Step: 4000... Loss: 1.4458...\n",
      "Epoch: 4/200... Step: 5000... Loss: 1.5459...\n",
      "Epoch: 5/200... Step: 6000... Loss: 1.6439...\n",
      "Epoch: 6/200... Step: 7000... Loss: 1.2206...\n",
      "Epoch: 7/200... Step: 8000... Loss: 0.9150...\n",
      "Epoch: 8/200... Step: 9000... Loss: 0.7754...\n",
      "Epoch: 8/200... Step: 10000... Loss: 1.0881...\n",
      "Epoch: 9/200... Step: 11000... Loss: 0.9381...\n",
      "Epoch: 10/200... Step: 12000... Loss: 0.9848...\n",
      "Epoch: 11/200... Step: 13000... Loss: 0.6031...\n",
      "Epoch: 12/200... Step: 14000... Loss: 0.7047...\n",
      "Epoch: 12/200... Step: 15000... Loss: 0.6862...\n",
      "Epoch: 13/200... Step: 16000... Loss: 0.7743...\n",
      "Epoch: 14/200... Step: 17000... Loss: 0.5336...\n",
      "Epoch: 15/200... Step: 18000... Loss: 0.6734...\n",
      "Epoch: 16/200... Step: 19000... Loss: 0.5369...\n",
      "Epoch: 16/200... Step: 20000... Loss: 0.5121...\n",
      "Epoch: 17/200... Step: 21000... Loss: 0.6138...\n",
      "Epoch: 18/200... Step: 22000... Loss: 0.6949...\n",
      "Epoch: 19/200... Step: 23000... Loss: 0.4147...\n",
      "Epoch: 20/200... Step: 24000... Loss: 0.4819...\n",
      "Epoch: 20/200... Step: 25000... Loss: 0.3996...\n",
      "Epoch: 21/200... Step: 26000... Loss: 0.4829...\n",
      "Epoch: 22/200... Step: 27000... Loss: 0.3472...\n",
      "Epoch: 23/200... Step: 28000... Loss: 0.3134...\n",
      "Epoch: 24/200... Step: 29000... Loss: 0.4510...\n",
      "Epoch: 24/200... Step: 30000... Loss: 0.4779...\n",
      "Epoch: 25/200... Step: 31000... Loss: 0.5417...\n",
      "Epoch: 26/200... Step: 32000... Loss: 0.2791...\n",
      "Epoch: 27/200... Step: 33000... Loss: 0.2826...\n",
      "Epoch: 28/200... Step: 34000... Loss: 0.3026...\n",
      "Epoch: 28/200... Step: 35000... Loss: 0.4716...\n",
      "Epoch: 29/200... Step: 36000... Loss: 0.4977...\n",
      "Epoch: 30/200... Step: 37000... Loss: 0.3416...\n",
      "Epoch: 31/200... Step: 38000... Loss: 0.2590...\n",
      "Epoch: 32/200... Step: 39000... Loss: 0.2774...\n",
      "Epoch: 32/200... Step: 40000... Loss: 0.3554...\n",
      "Epoch: 33/200... Step: 41000... Loss: 0.2627...\n",
      "Epoch: 34/200... Step: 42000... Loss: 0.8070...\n",
      "Epoch: 35/200... Step: 43000... Loss: 0.2247...\n",
      "Epoch: 36/200... Step: 44000... Loss: 0.2593...\n",
      "Epoch: 36/200... Step: 45000... Loss: 0.2539...\n",
      "Epoch: 37/200... Step: 46000... Loss: 0.2294...\n",
      "Epoch: 38/200... Step: 47000... Loss: 0.4005...\n",
      "Epoch: 39/200... Step: 48000... Loss: 0.2201...\n",
      "Epoch: 40/200... Step: 49000... Loss: 0.2433...\n",
      "Epoch: 40/200... Step: 50000... Loss: 0.3677...\n",
      "Epoch: 41/200... Step: 51000... Loss: 0.3589...\n",
      "Epoch: 42/200... Step: 52000... Loss: 0.2947...\n",
      "Epoch: 43/200... Step: 53000... Loss: 0.3961...\n",
      "Epoch: 44/200... Step: 54000... Loss: 0.4098...\n",
      "Epoch: 44/200... Step: 55000... Loss: 0.2991...\n",
      "Epoch: 45/200... Step: 56000... Loss: 0.6400...\n",
      "Epoch: 46/200... Step: 57000... Loss: 0.3200...\n",
      "Epoch: 47/200... Step: 58000... Loss: 0.1635...\n",
      "Epoch: 48/200... Step: 59000... Loss: 0.1917...\n",
      "Epoch: 48/200... Step: 60000... Loss: 0.2393...\n",
      "Epoch: 49/200... Step: 61000... Loss: 0.1706...\n",
      "Epoch: 50/200... Step: 62000... Loss: 0.1856...\n",
      "Epoch: 51/200... Step: 63000... Loss: 0.2795...\n",
      "Epoch: 52/200... Step: 64000... Loss: 0.5217...\n",
      "Epoch: 52/200... Step: 65000... Loss: 0.4542...\n",
      "Epoch: 53/200... Step: 66000... Loss: 0.2675...\n",
      "Epoch: 54/200... Step: 67000... Loss: 0.1421...\n",
      "Epoch: 55/200... Step: 68000... Loss: 0.1952...\n",
      "Epoch: 56/200... Step: 69000... Loss: 0.7312...\n",
      "Epoch: 56/200... Step: 70000... Loss: 0.2779...\n",
      "Epoch: 57/200... Step: 71000... Loss: 0.2012...\n",
      "Epoch: 58/200... Step: 72000... Loss: 0.2814...\n",
      "Epoch: 59/200... Step: 73000... Loss: 0.1180...\n",
      "Epoch: 60/200... Step: 74000... Loss: 0.0975...\n",
      "Epoch: 60/200... Step: 75000... Loss: 0.2810...\n",
      "Epoch: 61/200... Step: 76000... Loss: 0.3709...\n",
      "Epoch: 62/200... Step: 77000... Loss: 0.3582...\n",
      "Epoch: 63/200... Step: 78000... Loss: 0.0842...\n",
      "Epoch: 64/200... Step: 79000... Loss: 0.1511...\n",
      "Epoch: 64/200... Step: 80000... Loss: 0.3963...\n",
      "Epoch: 65/200... Step: 81000... Loss: 0.2227...\n",
      "Epoch: 66/200... Step: 82000... Loss: 0.1779...\n",
      "Epoch: 67/200... Step: 83000... Loss: 0.1258...\n",
      "Epoch: 68/200... Step: 84000... Loss: 0.1120...\n",
      "Epoch: 68/200... Step: 85000... Loss: 0.1699...\n",
      "Epoch: 69/200... Step: 86000... Loss: 0.2238...\n",
      "Epoch: 70/200... Step: 87000... Loss: 0.2120...\n",
      "Epoch: 71/200... Step: 88000... Loss: 0.1724...\n",
      "Epoch: 72/200... Step: 89000... Loss: 0.4066...\n",
      "Epoch: 72/200... Step: 90000... Loss: 0.1835...\n",
      "Epoch: 73/200... Step: 91000... Loss: 0.2451...\n",
      "Epoch: 74/200... Step: 92000... Loss: 0.2074...\n",
      "Epoch: 75/200... Step: 93000... Loss: 0.1739...\n",
      "Epoch: 76/200... Step: 94000... Loss: 0.1266...\n",
      "Epoch: 76/200... Step: 95000... Loss: 0.1124...\n",
      "Epoch: 77/200... Step: 96000... Loss: 0.1670...\n",
      "Epoch: 78/200... Step: 97000... Loss: 0.1173...\n",
      "Epoch: 79/200... Step: 98000... Loss: 0.2440...\n",
      "Epoch: 80/200... Step: 99000... Loss: 0.1666...\n",
      "Epoch: 80/200... Step: 100000... Loss: 0.1928...\n",
      "Epoch: 81/200... Step: 101000... Loss: 0.3894...\n",
      "Epoch: 82/200... Step: 102000... Loss: 0.5295...\n",
      "Epoch: 83/200... Step: 103000... Loss: 0.0775...\n",
      "Epoch: 84/200... Step: 104000... Loss: 0.2682...\n",
      "Epoch: 84/200... Step: 105000... Loss: 0.2925...\n",
      "Epoch: 85/200... Step: 106000... Loss: 0.1726...\n",
      "Epoch: 86/200... Step: 107000... Loss: 0.1658...\n",
      "Epoch: 87/200... Step: 108000... Loss: 0.1750...\n",
      "Epoch: 88/200... Step: 109000... Loss: 0.1054...\n",
      "Epoch: 88/200... Step: 110000... Loss: 0.2025...\n",
      "Epoch: 89/200... Step: 111000... Loss: 0.1665...\n",
      "Epoch: 90/200... Step: 112000... Loss: 0.0779...\n",
      "Epoch: 91/200... Step: 113000... Loss: 0.1989...\n",
      "Epoch: 92/200... Step: 114000... Loss: 0.1980...\n",
      "Epoch: 92/200... Step: 115000... Loss: 0.1347...\n",
      "Epoch: 93/200... Step: 116000... Loss: 0.1858...\n",
      "Epoch: 94/200... Step: 117000... Loss: 0.2609...\n",
      "Epoch: 95/200... Step: 118000... Loss: 0.2187...\n",
      "Epoch: 96/200... Step: 119000... Loss: 0.5582...\n",
      "Epoch: 96/200... Step: 120000... Loss: 0.0699...\n",
      "Epoch: 97/200... Step: 121000... Loss: 0.0759...\n",
      "Epoch: 98/200... Step: 122000... Loss: 0.3219...\n",
      "Epoch: 99/200... Step: 123000... Loss: 0.1927...\n",
      "Epoch: 100/200... Step: 124000... Loss: 0.0626...\n",
      "Epoch: 100/200... Step: 125000... Loss: 0.0932...\n",
      "Epoch: 101/200... Step: 126000... Loss: 0.0945...\n",
      "Epoch: 102/200... Step: 127000... Loss: 0.1557...\n",
      "Epoch: 103/200... Step: 128000... Loss: 0.4347...\n",
      "Epoch: 104/200... Step: 129000... Loss: 0.0816...\n",
      "Epoch: 104/200... Step: 130000... Loss: 0.2322...\n",
      "Epoch: 105/200... Step: 131000... Loss: 0.1400...\n",
      "Epoch: 106/200... Step: 132000... Loss: 0.1558...\n",
      "Epoch: 107/200... Step: 133000... Loss: 0.1548...\n",
      "Epoch: 108/200... Step: 134000... Loss: 0.0971...\n",
      "Epoch: 108/200... Step: 135000... Loss: 0.2902...\n",
      "Epoch: 109/200... Step: 136000... Loss: 0.1453...\n",
      "Epoch: 110/200... Step: 137000... Loss: 0.1275...\n",
      "Epoch: 111/200... Step: 138000... Loss: 0.1508...\n",
      "Epoch: 112/200... Step: 139000... Loss: 0.2354...\n",
      "Epoch: 112/200... Step: 140000... Loss: 0.0985...\n",
      "Epoch: 113/200... Step: 141000... Loss: 0.0733...\n",
      "Epoch: 114/200... Step: 142000... Loss: 0.0567...\n",
      "Epoch: 115/200... Step: 143000... Loss: 0.1623...\n",
      "Epoch: 116/200... Step: 144000... Loss: 0.3195...\n",
      "Epoch: 116/200... Step: 145000... Loss: 0.1895...\n",
      "Epoch: 117/200... Step: 146000... Loss: 0.1830...\n",
      "Epoch: 118/200... Step: 147000... Loss: 0.0685...\n",
      "Epoch: 119/200... Step: 148000... Loss: 0.2345...\n",
      "Epoch: 120/200... Step: 149000... Loss: 0.0506...\n",
      "Epoch: 120/200... Step: 150000... Loss: 0.2156...\n",
      "Epoch: 121/200... Step: 151000... Loss: 0.2143...\n",
      "Epoch: 122/200... Step: 152000... Loss: 0.4653...\n",
      "Epoch: 123/200... Step: 153000... Loss: 0.1708...\n",
      "Epoch: 124/200... Step: 154000... Loss: 0.1634...\n",
      "Epoch: 124/200... Step: 155000... Loss: 0.2942...\n",
      "Epoch: 125/200... Step: 156000... Loss: 0.4493...\n",
      "Epoch: 126/200... Step: 157000... Loss: 0.3547...\n",
      "Epoch: 127/200... Step: 158000... Loss: 0.2068...\n",
      "Epoch: 128/200... Step: 159000... Loss: 0.2157...\n",
      "Epoch: 128/200... Step: 160000... Loss: 0.0309...\n",
      "Epoch: 129/200... Step: 161000... Loss: 0.1068...\n",
      "Epoch: 130/200... Step: 162000... Loss: 0.1438...\n",
      "Epoch: 131/200... Step: 163000... Loss: 0.1571...\n",
      "Epoch: 132/200... Step: 164000... Loss: 0.0767...\n",
      "Epoch: 132/200... Step: 165000... Loss: 0.4116...\n",
      "Epoch: 133/200... Step: 166000... Loss: 0.2269...\n",
      "Epoch: 134/200... Step: 167000... Loss: 0.1702...\n",
      "Epoch: 135/200... Step: 168000... Loss: 0.2152...\n",
      "Epoch: 136/200... Step: 169000... Loss: 0.0745...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136/200... Step: 170000... Loss: 0.6293...\n",
      "Epoch: 137/200... Step: 171000... Loss: 0.2843...\n",
      "Epoch: 138/200... Step: 172000... Loss: 0.0619...\n",
      "Epoch: 139/200... Step: 173000... Loss: 0.0563...\n",
      "Epoch: 140/200... Step: 174000... Loss: 0.3814...\n",
      "Epoch: 140/200... Step: 175000... Loss: 0.3445...\n",
      "Epoch: 141/200... Step: 176000... Loss: 0.1387...\n",
      "Epoch: 142/200... Step: 177000... Loss: 0.2497...\n",
      "Epoch: 143/200... Step: 178000... Loss: 0.1744...\n",
      "Epoch: 144/200... Step: 179000... Loss: 0.1871...\n",
      "Epoch: 144/200... Step: 180000... Loss: 0.3278...\n",
      "Epoch: 145/200... Step: 181000... Loss: 0.2158...\n",
      "Epoch: 146/200... Step: 182000... Loss: 0.2493...\n",
      "Epoch: 147/200... Step: 183000... Loss: 0.1050...\n",
      "Epoch: 148/200... Step: 184000... Loss: 0.2431...\n",
      "Epoch: 148/200... Step: 185000... Loss: 0.1136...\n",
      "Epoch: 149/200... Step: 186000... Loss: 0.1674...\n",
      "Epoch: 150/200... Step: 187000... Loss: 0.2166...\n",
      "Epoch: 151/200... Step: 188000... Loss: 0.2013...\n",
      "Epoch: 152/200... Step: 189000... Loss: 0.0428...\n",
      "Epoch: 152/200... Step: 190000... Loss: 0.3607...\n",
      "Epoch: 153/200... Step: 191000... Loss: 0.2692...\n",
      "Epoch: 154/200... Step: 192000... Loss: 0.0833...\n",
      "Epoch: 155/200... Step: 193000... Loss: 0.2035...\n",
      "Epoch: 156/200... Step: 194000... Loss: 0.1163...\n",
      "Epoch: 156/200... Step: 195000... Loss: 0.2461...\n",
      "Epoch: 157/200... Step: 196000... Loss: 0.1058...\n",
      "Epoch: 158/200... Step: 197000... Loss: 0.1334...\n",
      "Epoch: 159/200... Step: 198000... Loss: 0.2279...\n",
      "Epoch: 160/200... Step: 199000... Loss: 0.3470...\n",
      "Epoch: 160/200... Step: 200000... Loss: 0.2580...\n",
      "Epoch: 161/200... Step: 201000... Loss: 0.1457...\n",
      "Epoch: 162/200... Step: 202000... Loss: 0.1079...\n",
      "Epoch: 163/200... Step: 203000... Loss: 0.1820...\n",
      "Epoch: 164/200... Step: 204000... Loss: 0.3426...\n",
      "Epoch: 164/200... Step: 205000... Loss: 0.1216...\n",
      "Epoch: 165/200... Step: 206000... Loss: 0.2088...\n",
      "Epoch: 166/200... Step: 207000... Loss: 0.1373...\n",
      "Epoch: 167/200... Step: 208000... Loss: 0.6653...\n",
      "Epoch: 168/200... Step: 209000... Loss: 0.1096...\n",
      "Epoch: 168/200... Step: 210000... Loss: 0.1459...\n",
      "Epoch: 169/200... Step: 211000... Loss: 0.1291...\n",
      "Epoch: 170/200... Step: 212000... Loss: 0.2679...\n",
      "Epoch: 171/200... Step: 213000... Loss: 0.3028...\n",
      "Epoch: 172/200... Step: 214000... Loss: 0.1092...\n",
      "Epoch: 172/200... Step: 215000... Loss: 0.0882...\n",
      "Epoch: 173/200... Step: 216000... Loss: 0.1674...\n",
      "Epoch: 174/200... Step: 217000... Loss: 0.2761...\n",
      "Epoch: 175/200... Step: 218000... Loss: 0.1146...\n",
      "Epoch: 176/200... Step: 219000... Loss: 0.0433...\n",
      "Epoch: 176/200... Step: 220000... Loss: 0.1399...\n",
      "Epoch: 177/200... Step: 221000... Loss: 0.0759...\n",
      "Epoch: 178/200... Step: 222000... Loss: 0.1761...\n",
      "Epoch: 179/200... Step: 223000... Loss: 0.1444...\n",
      "Epoch: 180/200... Step: 224000... Loss: 0.1168...\n",
      "Epoch: 180/200... Step: 225000... Loss: 0.3281...\n",
      "Epoch: 181/200... Step: 226000... Loss: 0.2193...\n",
      "Epoch: 182/200... Step: 227000... Loss: 0.2156...\n",
      "Epoch: 183/200... Step: 228000... Loss: 0.0769...\n",
      "Epoch: 184/200... Step: 229000... Loss: 0.1825...\n",
      "Epoch: 184/200... Step: 230000... Loss: 0.1371...\n",
      "Epoch: 185/200... Step: 231000... Loss: 0.1181...\n",
      "Epoch: 186/200... Step: 232000... Loss: 0.0389...\n",
      "Epoch: 187/200... Step: 233000... Loss: 0.0727...\n",
      "Epoch: 188/200... Step: 234000... Loss: 0.2106...\n",
      "Epoch: 188/200... Step: 235000... Loss: 0.1106...\n",
      "Epoch: 189/200... Step: 236000... Loss: 0.1520...\n",
      "Epoch: 190/200... Step: 237000... Loss: 0.2015...\n",
      "Epoch: 191/200... Step: 238000... Loss: 0.0518...\n",
      "Epoch: 192/200... Step: 239000... Loss: 0.0469...\n",
      "Epoch: 192/200... Step: 240000... Loss: 0.3474...\n",
      "Epoch: 193/200... Step: 241000... Loss: 0.1622...\n",
      "Epoch: 194/200... Step: 242000... Loss: 0.0608...\n",
      "Epoch: 195/200... Step: 243000... Loss: 0.0851...\n",
      "Epoch: 196/200... Step: 244000... Loss: 0.0922...\n",
      "Epoch: 196/200... Step: 245000... Loss: 0.0634...\n",
      "Epoch: 197/200... Step: 246000... Loss: 0.1277...\n",
      "Epoch: 198/200... Step: 247000... Loss: 0.3268...\n",
      "Epoch: 199/200... Step: 248000... Loss: 0.1046...\n",
      "Epoch: 200/200... Step: 249000... Loss: 0.2035...\n",
      "Epoch: 200/200... Step: 250000... Loss: 0.2511...\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "counter = 0\n",
    "index = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        counter += 1\n",
    "        \n",
    "        #if len(batch) != batch_size: continue\n",
    "        if(train_on_gpu):\n",
    "            inputs, targets = Variable(batch.content).cuda(), Variable(batch.sentiment).cuda()\n",
    "        else:\n",
    "            inputs, targets = batch.content, batch.sentiment\n",
    "        counter += 1\n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model(inputs)\n",
    "        #print(\"output: \", output.shape)\n",
    "        #print(\"targets: \", targets.shape)\n",
    "        \n",
    "        #loss = criterion(output, targets)\n",
    "        loss = F.cross_entropy(output, targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter % print_every == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filename = \"document_cls_text_cnn10.pth\"\n",
    "PATH = os.path.join(\"model\", filename)\n",
    "#torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = TextCNN(TEXT.vocab.vectors, TEXT.vocab.stoi[TEXT.pad_token], num_filters, kernel_sizes, label_size, 0.5)\n",
    "model = TextCNN(TEXT.vocab.vectors, TEXT.vocab.stoi[TEXT.pad_token], vocab_size, embidding_dim, num_filters, kernel_sizes, label_size, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = \"How are YOU convinced that I have always wanted you? What signals did I give off...damn I think I just lost another friend\"\n",
    "#sentence = \"The storm is here and the electricity is gone\"\n",
    "#sentence = \"Damm servers still down  i need to hit 80 before all the koxpers pass me\"\n",
    "#sentence = \"Need to pack for CALI CALI! Cannot waittt! Thinking a glass of wine is in order to celebrate my weekend vaca. Still work 2morrow, tho.\"\n",
    "#sentence = \"I'm worried I can do anything\"\n",
    "##sentence = \"I felt ecstatic when I passed my exam\"\n",
    "#sentence = \"I was overjoyed at the birth of my son.\"\n",
    "##sentence = \"During the Christmas holidays I felt wonderfully merry.\"\n",
    "#sentence = \"I’m feeling a little low at the moment.\"\n",
    "sentence = \"I was so annoyed when I failed my English test.\"\n",
    "sentence = \"Afraid of your own shadow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1280, 13, 38, 493, 8253]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [TEXT.vocab.stoi[word.lower()] for word in tokenizer(sentence)]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse = np.asarray(s)\n",
    "feature_tensor = torch.from_numpy(nse)\n",
    "feature_tensor = feature_tensor.unsqueeze(0)\n",
    "batch_size = feature_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "if(train_on_gpu):\n",
    "    feature_tensor = feature_tensor.cuda()\n",
    "    model.cuda()\n",
    "\n",
    "model.eval()\n",
    "print(feature_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2.3611,   8.2147,  -4.5427,  -1.5517,  -0.9713,   3.3712,  -8.3086,\n",
       "         -3.6140,  -3.4412,   2.0452,  -6.9978, -13.7870,  -5.4736],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(feature_tensor).squeeze()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#pred = torch.sigmoid(output.view(-1)).cpu().data.numpy().tolist()\n",
    "#loss = F.cross_entropy(logit, target, reduction='sum')\n",
    "pred = F.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, predicted = torch.max(output, 0)\n",
    "value = predicted.data.tolist()\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worry'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.itos[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral',\n",
       " 'worry',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'love',\n",
       " 'surprise',\n",
       " 'fun',\n",
       " 'relief',\n",
       " 'hate',\n",
       " 'empty',\n",
       " 'enthusiasm',\n",
       " 'boredom',\n",
       " 'anger']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.itos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
