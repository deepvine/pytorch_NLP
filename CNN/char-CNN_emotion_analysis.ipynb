{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char-CNN emotion analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import vocab\n",
    "from torchtext import data\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator\n",
    "from torchtext.vocab import GloVe\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    return x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "BLANK_WORD = \"<blank>\"\n",
    "max_vocab = 8000\n",
    "fix_length=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR = Field(sequential=True, tokenize=lambda x: list(x), pad_token=BLANK_WORD, lower=True, batch_first=True, fix_length=fix_length)\n",
    "LABEL = Field(sequential=False, unk_token=None, tokenize=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv\n",
    "train_data = TabularDataset(path='../data/text_emotion.csv', \n",
    "                            format='csv', \n",
    "                            skip_header=True,\n",
    "                            fields=[(\"tweet_id\", None),(\"sentiment\", LABEL),(\"author\", None),(\"content\",CHAR)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove = vocab.Vectors('../data/glove.6B.300d.txt')\n",
    "#tqdm_notebook().pandas() \n",
    "#https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(train_data, max_size=max_vocab)\n",
    "CHAR.build_vocab(train_data, min_freq=3)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function torchtext.vocab._default_unk_index()>,\n",
       "            {'<unk>': 0,\n",
       "             '<blank>': 1,\n",
       "             ' ': 2,\n",
       "             'e': 3,\n",
       "             't': 4,\n",
       "             'o': 5,\n",
       "             'a': 6,\n",
       "             'i': 7,\n",
       "             'n': 8,\n",
       "             's': 9,\n",
       "             'r': 10,\n",
       "             'h': 11,\n",
       "             'l': 12,\n",
       "             'd': 13,\n",
       "             'm': 14,\n",
       "             'u': 15,\n",
       "             'y': 16,\n",
       "             'g': 17,\n",
       "             'w': 18,\n",
       "             'c': 19,\n",
       "             '.': 20,\n",
       "             'p': 21,\n",
       "             'f': 22,\n",
       "             'b': 23,\n",
       "             'k': 24,\n",
       "             'v': 25,\n",
       "             '!': 26,\n",
       "             '@': 27,\n",
       "             \"'\": 28,\n",
       "             ',': 29,\n",
       "             'j': 30,\n",
       "             '?': 31,\n",
       "             '/': 32,\n",
       "             'x': 33,\n",
       "             'z': 34,\n",
       "             ';': 35,\n",
       "             '-': 36,\n",
       "             '&': 37,\n",
       "             ':': 38,\n",
       "             'q': 39,\n",
       "             '1': 40,\n",
       "             '2': 41,\n",
       "             '0': 42,\n",
       "             '3': 43,\n",
       "             '_': 44,\n",
       "             '4': 45,\n",
       "             '5': 46,\n",
       "             '6': 47,\n",
       "             '8': 48,\n",
       "             '7': 49,\n",
       "             ')': 50,\n",
       "             '9': 51,\n",
       "             '(': 52,\n",
       "             '*': 53,\n",
       "             '#': 54,\n",
       "             '½': 55,\n",
       "             '¿': 56,\n",
       "             'ï': 57,\n",
       "             '=': 58,\n",
       "             '~': 59,\n",
       "             '$': 60,\n",
       "             '+': 61,\n",
       "             ']': 62,\n",
       "             '^': 63,\n",
       "             '%': 64,\n",
       "             '[': 65,\n",
       "             '`': 66,\n",
       "             '|': 67,\n",
       "             '\\\\': 68,\n",
       "             '{': 69,\n",
       "             '}': 70,\n",
       "             'â': 71,\n",
       "             '\\xa0': 72})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@', 't', 'i']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_example = train_data.examples[0]\n",
    "one_example.content[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Iterator(train_data, \n",
    "                        batch_size=batch_size, \n",
    "                        #device=-1, \n",
    "                        repeat=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 30])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break;\n",
    "print(batch.content.shape)\n",
    "print(batch.sentiment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR.vocab.itos[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/CHAR.Field\",\"wb\")as f:\n",
    "     dill.dump(CHAR,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/LABEL_CHAR.Field\",\"wb\")as f2:\n",
    "     dill.dump(LABEL,f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(model, method='xavier', exclude='embedding', seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    for name, w in model.named_parameters():\n",
    "        if not exclude in name:\n",
    "            if 'weight' in name:\n",
    "                if method is 'xavier':\n",
    "                    nn.init.xavier_normal_(w)\n",
    "                elif method is 'kaiming':\n",
    "                    nn.init.kaiming_normal_(w)\n",
    "                else:\n",
    "                    nn.init.normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(w, 0.0)\n",
    "            else: \n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model, ignore='embedding'):\n",
    "    total = 0\n",
    "    for name, w in model.named_parameters():\n",
    "        if not ignore or ignore not in name:\n",
    "            total += w.nelement()\n",
    "            print('{} : {}  {} parameters'.format(name, w.shape, w.nelement()))\n",
    "    print('-------'*4)\n",
    "    print('Total {} parameters'.format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/CHAR.Field\",\"rb\")as f:\n",
    "     CHAR=dill.load(f)\n",
    "        \n",
    "with open(\"model/LABEL_CHAR.Field\",\"rb\")as f2:\n",
    "     LABEL=dill.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbatch_size=1024\\nepochs=200\\nembidding_dim = 300\\nseq_length = 50\\nvocab_size = len(TEXT.vocab.itos)\\nnum_filters = 128\\nkernel_sizes = [1,2,3,4,56,]\\nhidden_dim = 128 # hidden size of fully conntected layer\\nlabel_size = len(LABEL.vocab)\\nprint_every = 1000\\n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "batch_size=1024\n",
    "epochs=200\n",
    "embidding_dim = 300\n",
    "seq_length = 50\n",
    "vocab_size = len(TEXT.vocab.itos)\n",
    "num_filters = 128\n",
    "kernel_sizes = [1,2,3,4,56,]\n",
    "hidden_dim = 128 # hidden size of fully conntected layer\n",
    "label_size = len(LABEL.vocab)\n",
    "print_every = 1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab_size = len(CHAR.vocab.itos)\n",
    "char_embed_dim = 15\n",
    "word_vocab_size = len(LABEL.vocab.itos)\n",
    "word_embed_dim = 128\n",
    "\n",
    "kernel_widths = [1, 2, 3, 4, 5, 6]\n",
    "kernel_nums = [25, 50, 75, 100, 125, 150]\n",
    "dropout_prob = 0.5\n",
    "\n",
    "rnn_hidden = 300\n",
    "\n",
    "high_layers = 2\n",
    "lstm_num_layers = 2\n",
    "\n",
    "param_init = 0.05\n",
    "learning_rate_decay = 0.5\n",
    "decay_when = 1.0\n",
    "learning_rate = 1.0\n",
    "\n",
    "\n",
    "max_epoch = 25\n",
    "max_steps = 10000\n",
    "max_sent_len = 35\n",
    "max_word_len = 30\n",
    "clip = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Highway(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Highway, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size, bias=True)\n",
    "        self.fc2 = nn.Linear(input_size, input_size, bias=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        G = relu(x, Wg)\n",
    "        T = sigmoid(x, Wt)\n",
    "                                   |x, T == 0\n",
    "        y = G * T + x * (1. - T) = |\n",
    "                                   |G, T == 1\n",
    "        \"\"\"\n",
    "        t = F.sigmoid(self.fc1(x))\n",
    "        return torch.mul(t, F.relu(self.fc2(x))) + torch.mul(1-t, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    #num_filters = out-channels\n",
    "    def __init__(self, char_vocab_size, char_embed_dim, kernel_sizes, kernel_widths):\n",
    "        super(CharCNN, self).__init__()\n",
    "        \n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.kernel_widths = kernel_widths\n",
    "        \n",
    "        self.kernel = list(zip(kernel_sizes, kernel_widths))\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=out_channel, kernel_size=(char_embed_dim, filter_width)) \n",
    "                                      for out_channel, filter_width in self.kernel])\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pooled = []\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            conved = F.tanh(conv(x)).squeeze(3)\n",
    "            pooled.append(F.max_pool1d(conved, conved.shape[2]).squeeze(2))\n",
    "        \n",
    "        cat = torch.cat(cat, 1)\n",
    "        \n",
    "        return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN_LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN + highway network + LSTM\n",
    "    # Input: \n",
    "        4D tensor with shape [batch_size, in_channel, height, width]\n",
    "    # Output:\n",
    "        2D Tensor with shape [batch_size, vocab_size]\n",
    "    # Arguments:\n",
    "        char_emb_dim: the size of each character's embedding\n",
    "        word_emb_dim: the size of each word's embedding\n",
    "        vocab_size: num of unique words\n",
    "        num_char: num of characters\n",
    "        use_gpu: True or False\n",
    "    \"\"\"\n",
    "    def __init__(self, char_vocab_size, char_embed_dim, word_vocab_size, word_embed_dim, \n",
    "                 kernel_sizes, kernel_widths, lstm_num_layers, dropout_prob):\n",
    "        super(CharCNN_LSTM, self).__init__()\n",
    "        self.char_vocab_size = char_vocab_size\n",
    "        self.char_embed_dim = char_embed_dim\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        self.word_embed_dim = word_embed_dim\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.word_embed_dim = word_embed_dim\n",
    "        \n",
    "        self.char_embed = nn.Embedding(char_vocab_size, char_embed_dim, padding_idx=1)\n",
    "        self.conv = CharCNN(char_vocab_size, char_embed_dim, kernel_sizes, kernel_widths)\n",
    "        \n",
    "        # highway net\n",
    "        self.highway_input_dim = sum([x for x in kernel_widths])\n",
    "        self.batch_norm = nn.BatchNorm1d(self.highway_input_dim, affine=False)\n",
    "        self.highway1 = Highway(self.highway_input_dim)\n",
    "        self.highway2 = Highway(self.highway_input_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(input_size=self.highway_input_dim, \n",
    "                           hidden_size = self.word_embed_dim, \n",
    "                           num_layers = lstm_num_layers, \n",
    "                           dropout = dropout_prob,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # output layer\n",
    "        self.drop = nn.Dropout(dropout_prob)\n",
    "        self.linear = nn.Linear(self.word_embed_dim, self.word_vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.lstm_num_layers, batch_size, self.word_embed_dim).zero_().cuda(),\n",
    "                  weight.new(self.lstm_num_layers, batch_size, self.word_embed_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.lstm_num_layers, batch_size, self.word_embed_dim).zero_(),\n",
    "                      weight.new(self.lstm_num_layers, batch_size, self.word_embed_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x, hidden): # input <maxlen, batch, wdlen>\n",
    "        print(x.shape)\n",
    "        print(hidden[0].shape)\n",
    "        #lstm_batch_size = x.size()[0]\n",
    "        #lstm_seq_len = x.size()[1]\n",
    "        #max_word_len = x.size()[2]\n",
    "        #max_sent_len, batch, max_word_len = input.shape\n",
    "        \n",
    "        x_ = x.view(-1, max_word_len)\n",
    "\n",
    "        emb = self.char_embed(x_) # todo <maxlen * batch> --> <maxlen * batch, wembed>\n",
    "        #emb = self.drop(emb)\n",
    "        emb = emb.unsqueeze(1)\n",
    "        \n",
    "        #x = torch.transpose(x.view(x.size()[0], 1, x.size()[1], -1), 2, 3)\n",
    "\n",
    "        cnn = self.conv(emb) # <maxlen * batch, cedim> --> <maxlen * batch, cnn_size>\n",
    "\n",
    "        h_ = self.highway1(cnn) # todo <maxlen * batch, cnn_size> --> <maxlen * batch, cnn_size>\n",
    "        h_ = self.highway2(h_)\n",
    "        \n",
    "        h_ = h_.view(lstm_batch_size, batch, h_.size(-1)) # <maxlen, batch, cnn_size>\n",
    "        #x = x.contiguous().view(lstm_batch_size,lstm_seq_len, -1)\n",
    "        \n",
    "        output, hidden = self.lstm(h_, hidden) # todo <maxlen, batch, hdim>, <nlayer, batch, hdim>\n",
    "        output = self.drop(output)\n",
    "\n",
    "        decoded = output.view(output.size(0) * output.size(1), output.size(2)) # todo <maxlen * batch, hdim>\n",
    "        #x = x.contiguous().view(lstm_batch_size*lstm_seq_len, -1)\n",
    "        \n",
    "        decoded = self.linear(decoded) # todo <maxlen * batch, vsize>\n",
    "        #decoded = decoded.view(output.size(0), output.size(1), decoded.size(1)) # todo <maxlen, batch, vsize>\n",
    "\n",
    "        return decoded, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용할 수 있는지 확인\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')\n",
    "#train_on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharCNN_LSTM(\n",
       "  (char_embed): Embedding(73, 15, padding_idx=1)\n",
       "  (conv): CharCNN(\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv2d(1, 25, kernel_size=(15, 1), stride=(1, 1))\n",
       "      (1): Conv2d(1, 50, kernel_size=(15, 2), stride=(1, 1))\n",
       "      (2): Conv2d(1, 75, kernel_size=(15, 3), stride=(1, 1))\n",
       "      (3): Conv2d(1, 100, kernel_size=(15, 4), stride=(1, 1))\n",
       "      (4): Conv2d(1, 125, kernel_size=(15, 5), stride=(1, 1))\n",
       "      (5): Conv2d(1, 150, kernel_size=(15, 6), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (batch_norm): BatchNorm1d(21, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  (highway1): Highway(\n",
       "    (fc1): Linear(in_features=21, out_features=21, bias=True)\n",
       "    (fc2): Linear(in_features=21, out_features=21, bias=True)\n",
       "  )\n",
       "  (highway2): Highway(\n",
       "    (fc1): Linear(in_features=21, out_features=21, bias=True)\n",
       "    (fc2): Linear(in_features=21, out_features=21, bias=True)\n",
       "  )\n",
       "  (lstm): LSTM(21, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (linear): Linear(in_features=128, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharCNN_LSTM(char_vocab_size, char_embed_dim, word_vocab_size, word_embed_dim, \n",
    "                     kernel_nums, kernel_widths, lstm_num_layers, dropout_prob)\n",
    "init_network(model)\n",
    "if(train_on_gpu):\n",
    "    model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_embed.weight : torch.Size([73, 15])  1095 parameters\n",
      "conv.convs.0.weight : torch.Size([25, 1, 15, 1])  375 parameters\n",
      "conv.convs.0.bias : torch.Size([25])  25 parameters\n",
      "conv.convs.1.weight : torch.Size([50, 1, 15, 2])  1500 parameters\n",
      "conv.convs.1.bias : torch.Size([50])  50 parameters\n",
      "conv.convs.2.weight : torch.Size([75, 1, 15, 3])  3375 parameters\n",
      "conv.convs.2.bias : torch.Size([75])  75 parameters\n",
      "conv.convs.3.weight : torch.Size([100, 1, 15, 4])  6000 parameters\n",
      "conv.convs.3.bias : torch.Size([100])  100 parameters\n",
      "conv.convs.4.weight : torch.Size([125, 1, 15, 5])  9375 parameters\n",
      "conv.convs.4.bias : torch.Size([125])  125 parameters\n",
      "conv.convs.5.weight : torch.Size([150, 1, 15, 6])  13500 parameters\n",
      "conv.convs.5.bias : torch.Size([150])  150 parameters\n",
      "highway1.fc1.weight : torch.Size([21, 21])  441 parameters\n",
      "highway1.fc1.bias : torch.Size([21])  21 parameters\n",
      "highway1.fc2.weight : torch.Size([21, 21])  441 parameters\n",
      "highway1.fc2.bias : torch.Size([21])  21 parameters\n",
      "highway2.fc1.weight : torch.Size([21, 21])  441 parameters\n",
      "highway2.fc1.bias : torch.Size([21])  21 parameters\n",
      "highway2.fc2.weight : torch.Size([21, 21])  441 parameters\n",
      "highway2.fc2.bias : torch.Size([21])  21 parameters\n",
      "lstm.weight_ih_l0 : torch.Size([512, 21])  10752 parameters\n",
      "lstm.weight_hh_l0 : torch.Size([512, 128])  65536 parameters\n",
      "lstm.bias_ih_l0 : torch.Size([512])  512 parameters\n",
      "lstm.bias_hh_l0 : torch.Size([512])  512 parameters\n",
      "lstm.weight_ih_l1 : torch.Size([512, 128])  65536 parameters\n",
      "lstm.weight_hh_l1 : torch.Size([512, 128])  65536 parameters\n",
      "lstm.bias_ih_l1 : torch.Size([512])  512 parameters\n",
      "lstm.bias_hh_l1 : torch.Size([512])  512 parameters\n",
      "linear.weight : torch.Size([13, 128])  1664 parameters\n",
      "linear.bias : torch.Size([13])  13 parameters\n",
      "----------------------------\n",
      "Total 248678 parameters\n"
     ]
    }
   ],
   "source": [
    "#criterion = F.cross_entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3,momentum=0.8)\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                               lr = learning_rate, \n",
    "                               momentum=0.85)\n",
    "print_model(model, ignore=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 11,  3,  2, 19, 13,  2,  2, 21, 12,  6, 16,  3, 10,  2,  7,  8,  2,\n",
      "        14, 16,  2, 19,  6, 10,  2,  7,  9,  2, 23, 10], device='cuda:0')\n",
      "torch.Size([64, 30])\n",
      "torch.Size([2, 64, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional tensor, but got 4-dimensional tensor for argument #1 'self' (while checking arguments for max_pool1d)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-48e96db588da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(\"output: \", output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print(\"targets: \", targets.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-d3e073ac0415>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m#x = torch.transpose(x.view(x.size()[0], 1, x.size()[1], -1), 2, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <maxlen * batch, cedim> --> <maxlen * batch, cnn_size>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mh_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighway1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# todo <maxlen * batch, cnn_size> --> <maxlen * batch, cnn_size>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-64e4fca1ee49>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mconved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mpooled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconved\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# type: (Tensor, BroadcastingList1[int], Optional[BroadcastingList1[int]], BroadcastingList1[int], BroadcastingList1[int], bool, bool) -> Tensor  # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     return max_pool1d_with_indices(\n\u001b[0;32m--> 394\u001b[0;31m         input, kernel_size, stride, padding, dilation, ceil_mode)[0]\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m max_pool1d = torch._jit_internal.boolean_dispatch(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmax_pool1d_with_indices\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0m_stride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     return torch.max_pool1d_with_indices(\n\u001b[0;32m--> 386\u001b[0;31m         input, kernel_size, _stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional tensor, but got 4-dimensional tensor for argument #1 'self' (while checking arguments for max_pool1d)"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "counter = 0\n",
    "index = 0\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        counter += 1\n",
    "        \n",
    "        #if len(batch) != batch_size: continue\n",
    "        if(train_on_gpu):\n",
    "            inputs, targets = Variable(batch.content).cuda(), Variable(batch.sentiment).cuda()\n",
    "        else:\n",
    "            inputs, targets = batch.content, batch.sentiment\n",
    "        counter += 1\n",
    "        model.zero_grad()\n",
    "        \n",
    "        hidden_state = model.init_hidden(batch_size)\n",
    "        print(inputs[0])\n",
    "        output, hidden_state = model(inputs, hidden_state)\n",
    "        #print(\"output: \", output.shape)\n",
    "        #print(\"targets: \", targets.shape)\n",
    "        \n",
    "        output = logits.contiguous().view(-1, word_vocab_size)\n",
    "        \n",
    "        #loss = criterion(output, targets)\n",
    "        loss = F.cross_entropy(output, targets.view(-1))\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm(net.parameters(), 5, norm_type=2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter % print_every == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filename = \"document_cls_text_cnn10.pth\"\n",
    "PATH = os.path.join(\"model\", filename)\n",
    "#torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = TextCNN(TEXT.vocab.vectors, TEXT.vocab.stoi[TEXT.pad_token], num_filters, kernel_sizes, label_size, 0.5)\n",
    "model = TextCNN(TEXT.vocab.vectors, TEXT.vocab.stoi[TEXT.pad_token], vocab_size, embidding_dim, num_filters, kernel_sizes, label_size, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = \"How are YOU convinced that I have always wanted you? What signals did I give off...damn I think I just lost another friend\"\n",
    "#sentence = \"The storm is here and the electricity is gone\"\n",
    "#sentence = \"Damm servers still down  i need to hit 80 before all the koxpers pass me\"\n",
    "#sentence = \"Need to pack for CALI CALI! Cannot waittt! Thinking a glass of wine is in order to celebrate my weekend vaca. Still work 2morrow, tho.\"\n",
    "#sentence = \"I'm worried I can do anything\"\n",
    "##sentence = \"I felt ecstatic when I passed my exam\"\n",
    "#sentence = \"I was overjoyed at the birth of my son.\"\n",
    "sentence = \"During the Christmas holidays I felt wonderfully merry.\"\n",
    "#sentence = \"I’m feeling a little low at the moment.\"\n",
    "#sentence = \"I was so annoyed when I failed my English test.\"\n",
    "#sentence = \"Afraid of your own shadow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[828, 4, 3278, 1925, 2, 872, 0, 0]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [TEXT.vocab.stoi[word.lower()] for word in tokenizer(sentence)]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse = np.asarray(s)\n",
    "feature_tensor = torch.from_numpy(nse)\n",
    "feature_tensor = feature_tensor.unsqueeze(0)\n",
    "batch_size = feature_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "if(train_on_gpu):\n",
    "    feature_tensor = feature_tensor.cuda()\n",
    "    model.cuda()\n",
    "\n",
    "model.eval()\n",
    "print(feature_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3.4776,   3.9867,  -1.0742,   1.3718,   2.4690,  -5.4473,  -7.9056,\n",
       "        -15.8092, -10.7468,  -4.9786,  -8.9972, -10.3861, -16.7447],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(feature_tensor).squeeze()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3.4776,   3.9867,  -1.0742,   1.3718,   2.4690,  -5.4473,  -7.9056,\n",
       "        -15.8092, -10.7468,  -4.9786,  -8.9972, -10.3861, -16.7447],\n",
       "       grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pred = F.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "val,idx = pred.sort(descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.2631e-01, 3.1634e-01, 1.1538e-01, 3.8514e-02, 3.3369e-03, 6.7245e-05,\n",
       "        4.2082e-05, 3.6016e-06, 1.2089e-06, 3.0147e-07, 2.1017e-07, 1.3304e-09,\n",
       "        5.2203e-10], device='cuda:0', grad_fn=<SortBackward>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  0,  4,  3,  2,  9,  5,  6, 10, 11,  8,  7, 12], device='cuda:0')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5263\n",
      "0.3163\n",
      "0.1154\n",
      "0.0385\n",
      "0.0033\n",
      "0.0001\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "value = []\n",
    "for v in val.tolist():\n",
    "    v = round(v, 4)\n",
    "    value.append(v)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5263, 0.3163, 0.1154, 0.0385]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "4\n",
      "3\n",
      "2\n",
      "9\n",
      "5\n",
      "6\n",
      "10\n",
      "11\n",
      "8\n",
      "7\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "emotion = []\n",
    "for v in idx.tolist():\n",
    "    emotion.append(LABEL.vocab.itos[v])\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worry', 'neutral', 'love', 'sadness']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7fc776133d08>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = zip(emotion, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('worry', 0.5263)\n",
      "('neutral', 0.3163)\n",
      "('love', 0.1154)\n",
      "('sadness', 0.0385)\n",
      "('happiness', 0.0033)\n",
      "('empty', 0.0001)\n",
      "('surprise', 0.0)\n",
      "('fun', 0.0)\n",
      "('enthusiasm', 0.0)\n",
      "('boredom', 0.0)\n",
      "('hate', 0.0)\n",
      "('relief', 0.0)\n",
      "('anger', 0.0)\n"
     ]
    }
   ],
   "source": [
    "for z in result:\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "_, predicted = torch.max(output, 0)\n",
    "value = predicted.data.tolist()\n",
    "value\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.itos[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral',\n",
       " 'worry',\n",
       " 'happiness',\n",
       " 'sadness',\n",
       " 'love',\n",
       " 'surprise',\n",
       " 'fun',\n",
       " 'relief',\n",
       " 'hate',\n",
       " 'empty',\n",
       " 'enthusiasm',\n",
       " 'boredom',\n",
       " 'anger']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.itos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
